# -*- coding: utf-8 -*-
import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications.densenet import preprocess_input

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
import numpy as np
import os
import json
import matplotlib.pyplot as plt

# seed ê³ ì •
seed = 15
np.random.seed(seed)
tf.random.set_seed(seed)

# ì„¤ì •ê°’
BATCH_SIZE = 16
EPOCHS = 50
IMG_HEIGHT = 224
IMG_WIDTH = 224

# ğŸ”¹ ë¡œì»¬ ë°ì´í„°ì…‹ ê²½ë¡œ ì§ì ‘ ì§€ì •
# ì˜ˆ: dataset í´ë” ì•ˆì— train / validation ë””ë ‰í† ë¦¬ ì¡´ì¬í•´ì•¼ í•¨
base_dir = "C:/Users/itg/Pictures/wheat"   # ğŸ‘‰ ì—¬ê¸°ë¥¼ ë³¸ì¸ PC ê²½ë¡œë¡œ ë°”ê¿”ì£¼ì„¸ìš”
train_dir = os.path.join(base_dir, "train")
validation_dir = os.path.join(base_dir, "validation")

# 1ï¸âƒ£ image_dataset_from_directoryë¡œ ë°ì´í„° ë¡œë“œ
train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    label_mode='categorical',
    batch_size=BATCH_SIZE,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    validation_dir,
    label_mode='categorical',
    batch_size=BATCH_SIZE,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    shuffle=False
)

# 2ï¸âƒ£ ë°ì´í„° ì¦ê°• í•¨ìˆ˜
def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=0.2)
    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)
    image = tf.image.random_hue(image, max_delta=0.05)
    return image, label

# 3ï¸âƒ£ ë¹„ìœ¨ ìœ ì§€ + íŒ¨ë”© + ì „ì²˜ë¦¬
def resize_pad_preprocess(image, label):
    image = tf.image.resize_with_pad(image, IMG_HEIGHT, IMG_WIDTH)
    image = preprocess_input(image)
    return image, label

# í´ë˜ìŠ¤ ì´ë¦„ ì €ì¥ìš©
class_list = train_ds.class_names

# 4ï¸âƒ£ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
AUTOTUNE = tf.data.AUTOTUNE
train_ds = (train_ds
            .map(augment, num_parallel_calls=AUTOTUNE)
            .map(resize_pad_preprocess, num_parallel_calls=AUTOTUNE)
            .apply(tf.data.experimental.ignore_errors())
            .cache()
            .prefetch(AUTOTUNE))

val_ds = (val_ds
          .map(resize_pad_preprocess, num_parallel_calls=AUTOTUNE)
          .apply(tf.data.experimental.ignore_errors())
          .cache()
          .prefetch(AUTOTUNE))

# 5ï¸âƒ£ DenseNet121 ë¶ˆëŸ¬ì˜¤ê¸°
base_model = DenseNet121(weights="imagenet", include_top=False,
                         input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))

# ì „ì´í•™ìŠµ: ê¸°ë³¸ ê°€ì¤‘ì¹˜ëŠ” freeze
for layer in base_model.layers:
    layer.trainable = False

# 6ï¸âƒ£ ì¶œë ¥ì¸µ ì¶”ê°€
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(64, activation="relu")(x)
x = Dropout(0.5)(x)
output_layer = Dense(len(class_list), activation="softmax")(x)

model = Model(inputs=base_model.input, outputs=output_layer)
model.summary()

# 7ï¸âƒ£ ì»´íŒŒì¼
loss_fn = CategoricalCrossentropy(label_smoothing=0.1)
model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss=loss_fn,
    metrics=["accuracy"]
)

# 8ï¸âƒ£ í•™ìŠµ
early_stop = EarlyStopping(monitor="val_loss", patience=5,
                           restore_best_weights=True)

history = model.fit(
    train_ds,
    epochs=EPOCHS,
    validation_data=val_ds,
    callbacks=[early_stop]
)

# 9ï¸âƒ£ í•™ìŠµ ê²°ê³¼ ê·¸ë˜í”„
plt.plot(history.history["accuracy"], label="train_accuracy")
plt.plot(history.history["val_accuracy"], label="val_accuracy")
plt.title("Model Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.title("Model Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# ğŸ”Ÿ ëª¨ë¸ + í´ë˜ìŠ¤ ì €ì¥ (ë¡œì»¬ PCì— ì €ì¥)
save_dir = "models"  # ğŸ‘‰ ì›í•˜ëŠ” ì €ì¥ ê²½ë¡œë¡œ ë³€ê²½
os.makedirs(save_dir, exist_ok=True)

save_model_path = os.path.join(save_dir, "Densenet_Xray.h5")
save_label_path = os.path.join(save_dir, "Densenet_Xray.json")

model.save(save_model_path)
with open(save_label_path, "w") as f:
    json.dump(class_list, f)

print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_model_path}")
print(f"âœ… í´ë˜ìŠ¤ ì €ì¥ ì™„ë£Œ: {save_label_path}")
